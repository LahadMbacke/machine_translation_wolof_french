{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC83scb3ZN4D"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "QFiJAhB9Zz3A",
        "outputId": "0cb07fc4-d933-473f-df10-81dcb6ee89c6"
      },
      "outputs": [],
      "source": [
        "# # !pip install transformers datasets\n",
        "# # from datasets import load_dataset\n",
        "# from google.colab import drive\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "data = pd.read_csv(\"data/train.csv\")\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc_MZXi_iHha"
      },
      "outputs": [],
      "source": [
        "data.head()\n",
        "wolof = data[\"WOLOF\"]\n",
        "francais = data[\"FRENCH\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyaWRJe9ijc_"
      },
      "outputs": [],
      "source": [
        "wolof_sentences = [sentence.rstrip('\\n').lower() for sentence in wolof]\n",
        "francais_sentences = [sentence.rstrip('\\n') for sentence in francais]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxpTRLmzjWbg",
        "outputId": "5320b4b7-68a3-426b-d5d5-9023654c98d5"
      },
      "outputs": [],
      "source": [
        "wolof_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7zkFA3plKI_",
        "outputId": "5119dcf0-c07a-49f1-edd7-63d1862fce52"
      },
      "outputs": [],
      "source": [
        "francais_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oDSPNB_nlrvv"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator, TabularDataset\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import spacy\n",
        "import stanza\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c06d678f21644b20b2fdcdb3500e7410",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-25 18:38:32 INFO: Downloaded file to /home/lahad/stanza_resources/resources.json\n",
            "2024-04-25 18:38:32 INFO: Downloading default packages for language: wo (Wolof) ...\n",
            "2024-04-25 18:38:33 INFO: File exists: /home/lahad/stanza_resources/wo/default.zip\n",
            "2024-04-25 18:38:33 INFO: Finished downloading models and saved to /home/lahad/stanza_resources\n",
            "2024-04-25 18:38:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28636fcfdb794fbab9aa85e153a84702",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-25 18:38:33 INFO: Downloaded file to /home/lahad/stanza_resources/resources.json\n",
            "2024-04-25 18:38:33 WARNING: Language wo package default expects mwt, which has been added\n",
            "2024-04-25 18:38:33 INFO: Loading these models for language: wo (Wolof):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | wtb     |\n",
            "| mwt       | wtb     |\n",
            "=======================\n",
            "\n",
            "2024-04-25 18:38:33 INFO: Using device: cpu\n",
            "2024-04-25 18:38:33 INFO: Loading: tokenize\n",
            "2024-04-25 18:38:33 INFO: Loading: mwt\n",
            "2024-04-25 18:38:33 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "spacy_frenc = spacy.load(\"fr_core_news_sm\")\n",
        "stanza.download('wo')\n",
        "\n",
        "# Initialiser le pipeline de traitement avec tokenisation\n",
        "stanza_wolof = stanza.Pipeline(lang='wo', processors='tokenize')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dafa', 'am', 'solo', 'motax', '.', 'ndax', 'dinga', 'dém', 'si', 'biir', '?']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_wolof(text):\n",
        "    doc = stanza_wolof(text)\n",
        "    tokens = [token.text for sentence in doc.sentences for token in sentence.tokens]\n",
        "    return tokens\n",
        "text = \"Dafa am solo motax. ndax dinga dém si biir?\"\n",
        "# tokenize_wolof(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Je',\n",
              " 'suis',\n",
              " 'une',\n",
              " 'étudiante',\n",
              " '.',\n",
              " 'Est',\n",
              " '-ce',\n",
              " 'que',\n",
              " 'tu',\n",
              " 'viens',\n",
              " 'avec',\n",
              " 'moi',\n",
              " '?']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_french(text):\n",
        "    doc = spacy_frenc(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "text = \"Je suis une étudiante. Est-ce que tu viens avec moi?\"\n",
        "# tokenize_french(text)  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
